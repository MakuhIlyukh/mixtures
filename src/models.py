# %%
from typing import Iterable

import torch
from torch import nn
from torch.nn.utils import parametrize
from geotorch import positive_semidefinite
from tqdm import tqdm

from src.initializers import DirichletInitializer, init_factory


class SoftmaxParametrization(torch.nn.Module):
    def __init__(self, c=0):
        super().__init__()
        self.c = c

    def forward(self, X):
        return nn.functional.softmax(X, dim=-1)
    
    def right_inverse(self, Y):
        return torch.log(Y) + self.c


class GM(torch.nn.Module):
    def __init__(self,
                 k, d, c=0):
        super().__init__()

        # means
        self.m_w = torch.nn.Parameter(
            torch.randn((k, d), requires_grad=True, dtype=torch.float64))
        
        # inversed covs
        self.l_w = torch.nn.Parameter(
            torch.empty((k, d, d), requires_grad=True, dtype=torch.float64))
        positive_semidefinite(self, "l_w")
        with torch.no_grad():
            self.l_w.data = torch.randn((k, d, d))
        
        # mix probs
        self.p_w = torch.nn.Parameter(
            torch.empty(k, dtype=torch.float64, requires_grad=True))
        parametrize.register_parametrization(
            self, 'p_w', SoftmaxParametrization(c))
        self.p_w = torch.distributions.Dirichlet(
                concentration=torch.full((k,), 1.0, dtype=torch.float64)
            ).sample()
        
        # constants
        self._mult_const = (torch.pi * 2)**(-d / 2)
        self.k = k
        self.d = d

    def forward(self, x):
        s = torch.zeros((x.shape[0], 1), dtype=torch.float64)
        # TODO: можно ли не пробегаться по всем k?
        for k in range(self.k):
            x_cent = (x - self.m_w[k])
            # TODO: можно избавиться от суммирования, если изменить порядок операций?
            s += (
                self.p_w[k]
                * self._mult_const
                * torch.sqrt(torch.det(self.l_w[k]))
                * torch.exp(
                    - torch.sum(
                        x_cent * (x_cent @ self.l_w[k]),
                        dim=1,
                        keepdim=True)
                    / 2
                ))
        return s

    def s_w(self, detach=True):
        if detach:
            return torch.linalg.inv(self.l_w.detach())
        else:
            return torch.linalg.inv(self.l_w) 



# ===============================================
# SEED = 146
# N_EPOCHS = 10000
# # learning rate
# LR = 0.00001
# # softmax_inverse(p) = log(p) + C
# C = 0 


# np.random.seed(SEED)
# torch.manual_seed(SEED)


# class SoftmaxParametrization(torch.nn.Module):
#     def __init__(self, c=0):
#         super().__init__()
#         self.c = c

#     def forward(self, X):
#         return nn.functional.softmax(X, dim=-1)
    
#     def right_inverse(self, Y):
#         return torch.log(Y) + self.c


# class GM(torch.nn.Module):
#     def __init__(self, c=0):
#         super().__init__()
#         # means
#         self.m_w = torch.nn.Parameter(
#             torch.tensor(np.random.randn(K, D) * 10, requires_grad=True))
#         # inversed covs
#         l_w = np.empty((K, D, D))
#         for i in range(K):
#             l_w[i] = make_spd_matrix(D, random_state=i + SEED)
#         self.l_w = torch.nn.Parameter(
#             torch.tensor(l_w, requires_grad=True))
#         positive_semidefinite(self, "l_w")
#         # self.l_w = torch.tensor(l_w, requires_grad=True)
#         # pk = softmax(logits_w)
#         # TODO: why logits generated by dirichlet???
#         self.p_w = torch.nn.Parameter(torch.empty(K, dtype=torch.float64, requires_grad=True))
#         parametrize.register_parametrization(self, 'p_w', SoftmaxParametrization(c))
#         self.p_w = torch.tensor(np.random.dirichlet(alpha=[1] * K), requires_grad=True)
#         # constant
#         self._mult_const = (torch.pi * 2)**(-D / 2)

#     def forward(self, x):
#         s = torch.zeros((x.shape[0], 1), dtype=torch.float64)
#         # if __debug__:
#         #     print('softmaxed:', softmaxed)
#         # TODO: можно ли не пробегаться по всем k?
#         for k in range(K):
#             x_cent = (x - self.m_w[k])
#             # if __debug__:
#                 # print(k)
#                 # print('det:', torch.det(self.l_w[k]))
#                 # print('exp:', torch.exp(- torch.sum((x_cent * (x_cent @ self.l_w[k])), dim=1, keepdim=True) / 2))
#                 # print('+=:', softmaxed[k] * self._mult_const * torch.det(self.l_w[k]) * torch.exp(
#                     # - torch.sum((x_cent * (x_cent @ self.l_w[k])), dim=1, keepdim=True)
#                     # / 2
#                 # ))
#                 # print('s:', s)
#             # TODO: можно избавиться от суммирования, если изменить порядок операций?
#             s += self.p_w[k] * self._mult_const * torch.det(self.l_w[k]) * torch.exp(
#                 - torch.sum((x_cent * (x_cent @ self.l_w[k])), dim=1, keepdim=True)
#                 / 2
#             )
#         return s


# def loss_fn(out):
#     return torch.sum(-torch.log(out))


# gm = GM(c=C)
# optimizer = torch.optim.Adam(gm.parameters(), lr=LR)

# # TODO: может надо делать стандартизацию перед обучением???
# pbar = tqdm(range(N_EPOCHS))
# X_tensor = torch.tensor(X)
# cs = {'p_w grad': [], 'l_w grad': [], 'm_w grad': [], 'loss': []}
# for epoch in pbar:
#     # if __debug__:
#     #     print(gm.l_w)
#     #     print('loss:', loss_fn(gm(torch.tensor(X[:10]))))
#     optimizer.zero_grad()
#     likelihood = gm(X_tensor)  # TODO: only X[:10]!!!
#     loss = loss_fn(likelihood)  
#     # if __debug__:
#     #     print('loss:', loss)
#     #     print('likelihood', likelihood)
#     #     print('-log likelihood', -torch.log(likelihood))
#     loss.backward()
#     optimizer.step()
#     pbar.set_postfix({'loss': loss.item()})
#     cs['p_w grad'].append(torch.max(torch.abs(gm.parametrizations.p_w.original.grad)).item())
#     cs['m_w grad'].append(torch.max(torch.abs(gm.m_w.grad)).item())
#     cs['l_w grad'].append(torch.max(torch.abs(gm.parametrizations.l_w.original.grad)).item())
#     cs['loss'].append(loss.item())

# print(gm.p_w.detach().numpy())
# print(PS)